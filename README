Right now, everything is running on Cloudera VM.

Steps to get working on Cloudera VM:
sudo apt-get install python-numpy
sudo apt-get install python-tables
python ./display_song.py -summary msd_summary_file.h5

Steps to get working on Starcluster AMI:
install hdf5
apt-get remove cython
install cython
install numexpr
export HDF5_DIR
export LD_LIBRARY_PATH
install pytables

Times:
2 small most_popular_terms 3:52:56
4 small most_popular_terms 2:26:21
upload to hdfs: 10:59-12:47 for 3/26
old add_database 1:01:38 for 7.01%
new add_database 25:00 for 7.01%
new add_database 1:00:52 for 14.63%
new add_database 6:54:24 for complete

after add_database may have to reboot cluster
due to starcluster error, have to reboot twice

Speedup lessons:
hadoop fs -copyFromLocal not necessarily best option
numpy significantly faster than python
use global: on build_bayes, speed up mapper from 54 minutes to 7:30
dictionary.has_key(key) is way way faster than key in dictionary.keys()

Annoyances:
garbage collection failure

total data size ~ 3GB

building additionalfiles
get test artist split from https://github.com/tb2332/MSongsDB/raw/master/Tasks_Demos/Tagging/artists_train.txt
get cover song shs_dataset_test and _train from https://github.com/tb2332/MSongsDB/raw/master/Tasks_Demos/CoverSongs/shs_dataset_train.txt and https://github.com/tb2332/MSongsDB/raw/master/Tasks_Demos/CoverSongs/shs_dataset_test.txt